{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sudi7513/classification-with-various-models?scriptVersionId=107495236\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"\n<p style=\"font-family: Arial; font-size:2em;color:green; font-style:bold\"><br>\n    Income Classification </p>\n    <p style=\"font-family: Arial;color:black; font-style:bold\">Hello,I have worked on the Adult Income daatset. This notebook is based on classifying the income into two groups.This is a dichotomous classification notebook. The goal is to predict whether a certain individual earns more or less than $50,000. </p>\n    <p>This notebook has the following sections and subsections</p><br>\n    \n <ul >\n <li>IMPORTING LIBRARIES</li>\n <li>CLEANING THE DATA</li>\n <ul><li>Level 1 feature selection using chi-square</li></ul>\n <li>ENCODING THE CATEGORICAL VARIABLES</li>\n <ul><li>Normalizing the variables</li>\n <li>Level 2 feature selection using chi square </li>\n <li>Heatmap of all selected variables</li></ul>\n <li>EXPLORATORY DATA ANALYSIS</li>\n <li>FITTING APPROPRIATE MODEL</li>\n <ul><li>Logistic Regression</li>\n <li>Decision Tree</li>\n <li>Naive Bayes</li>\n <li>Random Forest</li></ul>\n <li>COMPARING ALL MODELS BY THEIR ROC</li>\n\n   ","metadata":{}},{"cell_type":"markdown","source":"<h2>1. IMPORTING LIBRARIES</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>2. CLEANING THE DATA</h2>","metadata":{}},{"cell_type":"markdown","source":"First I will import and visualize the data.","metadata":{}},{"cell_type":"code","source":"adult=pd.read_csv('../input/adult-census-income/adult.csv')\nprint(adult.head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The missing values are shows as '?'. Hence I will replace them with NAN values.","metadata":{}},{"cell_type":"code","source":"adult[adult=='?']=np.nan\nprint(adult.isnull().sum(),'\\n')\nprint('Dimensions:',adult.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset has 15 columns. The missing values are ony present in the categorical columns. Several predictive supervise models like KNN or other methods can be used to predict these values and impute them. I will remove them in this case.","metadata":{}},{"cell_type":"code","source":"adult_new=adult.dropna(axis=0)\nadult_new.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col=adult_new.columns\ndata_type=adult_new.dtypes\nuniq=adult_new.nunique()\n\nprint(\"\\n%30s  %10s   %10s\\n \" % (\"Column Name\", \"Data Type\", \"Unique Values\"))\nfor i in range(len(adult_new.columns)):\n    print(\"%30s  %10s   %10s \" % (col[i],data_type[i],uniq[i]))\n\nprint(\"\\nDimensions:\",adult_new.shape[0],'rows and ',adult_new.shape[1],'columns')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replace the dependent column into appropriate format for fitting models.\nThe education.num column is the numerical representation of education column and hence redundant.","metadata":{}},{"cell_type":"code","source":"\nadult_new['income'].replace({'<=50K':0,'>50K':1},inplace=True)\nadult_new=adult_new.drop('education.num',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualising the various values under the categorical columns.\n","metadata":{}},{"cell_type":"code","source":"from collections import Counter\noccupatn=dict(Counter(adult_new['occupation'])).keys()\nprint('Occupation types:','\\n',list(occupatn),'\\n')\nrace=dict(Counter(adult_new['race'])).keys()\nprint('Race types:','\\n',list(race),'\\n')\nrelation=dict(Counter(adult_new['relationship'])).keys()\nprint('Relation types:','\\n',list(relation),'\\n')\neducate=dict(Counter(adult_new['education'])).keys()\nprint('Education levels:','\\n',list(educate),'\\n')\nmarital=dict(Counter(adult_new['marital.status'])).keys()\nprint('Marital status levels:','\\n',list(marital),'\\n')\nwork=dict(Counter(adult_new['workclass'])).keys()\nprint('Workclass levels:','\\n',list(work),'\\n')\ncountry=dict(Counter(adult_new['native.country'])).keys()\nprint('Native countries:','\\n',list(country),'\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding the correlation of all the other variables with the dependent variable. I will create a heatmap for the same.\nLook at the last row which is the 'income' and check the correlation. The variables which seem to have no or negative correlation with the dependent variable, I will drop it.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = adult_new.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(adult_new[top_corr_features].corr(),annot=True,cmap=\"twilight_shifted_r\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The range of correlation is from -1 to +1. On the right hand side there is a reference given so that by the colour on the heatmap the correlation can be easily determined. Here, 'fnlwgt' correlate nagatively with the 'income' and hence I will drop it. Also, in a correlation heatmap all the diagonal elements will be 1 indicating high correlation as they correlate with themselves.","metadata":{}},{"cell_type":"markdown","source":"A statistical test called point biserial correlation is used to measure the relationship between a binary variable x and a continuous variable y. It works on the same concept of a correlation coefficient. I will print the correlation coefficient to check which variables to drop. Since it works only between continuous and binary variables, I will use this for the 5 continuous variables in the dataset.","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\na=['age','capital.loss','capital.gain','hours.per.week','fnlwgt']\nfor i in a:\n    print(i,':',stats.pointbiserialr(adult_new['income'],adult_new[i])[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As above, the 'fnlwgt' has a negative correation with 'income' and hence I will drop the column and the remaining varibales are such.","metadata":{}},{"cell_type":"code","source":"adult_new=adult_new.drop('fnlwgt',axis=1)\nadult_new.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_cols = adult_new.columns[adult_new.dtypes==object].tolist()\ncategorical_cols","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><h3>2.1 Level 1 feature selection using chi square estimate</h3></p>","metadata":{}},{"cell_type":"markdown","source":"To measure the correlation between 2 categorical variables, I will use the chi-square estimate. The chi-square gives me a contingency table and calculates the p-value and the chi-square estimate. \nThe hypothesis of the chi-square test is \n\nH0: variable not related.\n\nH1: variables related.\n\nI set the value of alpha as 0.01. If the p-value is less than alpha, I will reject H0 and hence the variables are related.","metadata":{}},{"cell_type":"code","source":"\ndef cross_tab(obs1=[]):\n    observed=pd.crosstab(obs1,adult_new['income'])\n    val=stats.chi2_contingency(observed)\n    return(val[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpha=0.01\ndf=adult_new.drop('income',axis=1)\ncount=0\nattributes2=[]\nfor i in categorical_cols:\n    p_value=cross_tab(adult_new[i])\n    if p_value<=alpha:\n        count+=1\n        attributes2.append(i)\nprint('Number of attributes contributing:',count,'\\n')\nprint(attributes2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I see that all the categorical variables contribute to 'income'. Hence, I keep all of them.","metadata":{}},{"cell_type":"markdown","source":"Below is how a contingency table looks like.","metadata":{}},{"cell_type":"code","source":"pd.crosstab(adult.relationship,adult_new['income'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><p>3. ENCODING THE CATEGORICAL VARIABLES</p></h3>","metadata":{}},{"cell_type":"code","source":"categorical_cols","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fitting the categorical columns into models without encoding them will be a problem and hence I encode them using get_dummies. It gives me the dummy variables. For eg- if race has 4 types, for each row one type will have '1' under its category and '0' under the others. Thus, all the categorical variables will split into their respective 'n' types. This process can also be done using OneHotEncoder or LabelEncoder","metadata":{}},{"cell_type":"code","source":"adult_new1=pd.get_dummies(adult_new,columns=categorical_cols)\nadult_new1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adult_new1.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, I have 103 columns. Fitting this huge set of attributes is not recommended and hence the next step is feature selection. Also, since almost all varibales are either 0 or 1, we normalize the continuous variables to be between 0 and 1.","metadata":{}},{"cell_type":"markdown","source":"<p><h3>3.1 Normalizing the variables</h3></p>","metadata":{}},{"cell_type":"markdown","source":"I use the min max normalization which by default scales all the variables between 0 and 1 but the range can be specified for other scales.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\ncolumns_to_scale = ['age', 'capital.gain', 'capital.loss', 'hours.per.week']\nmms = MinMaxScaler()\nmin_max_scaled_columns = mms.fit_transform(adult_new1[columns_to_scale])\n#processed_data = np.concatenate([min_max_scaled_columns, adult_new], axis=1)\nadult_new1['age'],adult_new1['capital.gain'],adult_new1['capital.loss'],adult_new1['hours.per.week']=min_max_scaled_columns[:,0],min_max_scaled_columns[:,1],min_max_scaled_columns[:,2],min_max_scaled_columns[:,3]\nadult_new1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><h3>3.2 Level 2 feature selection using chi square estimate</h3></p>","metadata":{}},{"cell_type":"code","source":"category=adult_new1.columns[adult_new1.dtypes!=object].tolist()[5:]\n#category\nalpha=0.01\n#df=adult_new.drop('income',axis=1)\ncount=0\nfeatures=[]\nfor i in category:\n    p_value=cross_tab(adult_new1[i])\n    if p_value<=alpha:\n        count+=1\n        features.append(i)\n        #print(i,' has a relation')\n        #print('p-value for ',i,' is ',cross_tab(adult_new[i]),'\\n')\nprint('Number of contributing attributes:',count,'\\n')\nprint(features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to chi square test, only 60 attributes contribute to the dependent variable. I add the continuous variables to the set of selected attributes.","metadata":{}},{"cell_type":"code","source":"features.append('age')\nfeatures.append('capital.gain')\nfeatures.append('capital.loss')\nfeatures.append('hours.per.week')\nfeatures.append('income')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adult_new1[features].head()\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><p>3.3 Heatmap of all selected variables</p></h3>\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = adult_new1[features].corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(adult_new1[top_corr_features].corr(),annot=True,cmap=\"twilight_shifted_r\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><h3>4. EXPLORATORY DATA ANALYSIS</h3></p>\n","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,10))\n#plt.figure(figsize=(7,10))\nincome1=adult_new1['income'].value_counts()\nax[0].pie(income1,explode=(0,0.05),autopct='%1.1f%%',startangle=90,labels=['<=50K','>50K'])\nax[0].set_title('Income Share')\nax[1]=sns.countplot(x='income',data=adult_new1,palette='pastel')\nax[1].legend(labels=['<=50K','>50K'])\nax[1].set(xlabel=\"INCOME CATEGORIES\")\nax[1].set(ylabel='COUNT OF THE CATEGORIES')\nax[1].set_title('COUNT OF THE TWO LEVELS')\n\nfor p in ax[1].patches:\n    ax[1].annotate(p.get_height(),(p.get_x()+0.3,p.get_height()+500))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>There is a total of 30162 observations with 7508 people earning more than 50K and 22654 earning less than or equal to 50K.\n</p>\n<p>\nThe percentage division can be clearly understood by the pie chart.</p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nax=sns.countplot(x='income',hue='sex',data=adult_new,palette='Set1')\nax.set(xlabel='INCOME 50')\nax.set(ylabel='COUNT WITH AGE')\nax.set_title('INCOME WITH RESPECT TO SEX')\nfor p in ax.patches:\n    ax.annotate(p.get_height(),(p.get_x()+0.15,p.get_height()+200))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>15% women are in >50 category and the rest are 85% men</p>\n<p>38% women are in <=50 and the rest are 62% men</p>\n<p>69% of all men earn <=50K while only 31% earn more than 50K</p>\n<p>Only 10% of all women in the sample earn more than 50K while the rest 90% earn less than 50K</p>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 8))\nax = sns.countplot(x=\"income\", hue=\"race\", data=adult_new, palette=\"Set1\")\nax.set_title(\"FREQUENCY DISTRIBUTION OF INCOME WITH RESPECT TO AGE\")\nax.set(xlabel='INCOME RANGE',ylabel='FREQUENCY OF AGES')\n\nfor p in ax.patches:\n    ax.annotate(p.get_height(),(p.get_x()+0.05,p.get_height()+200))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"income\", data=adult_new, palette=\"Set2\")\nax.set_title(\"FREQUENCY DISTRIBUTION OF WORKCLASS WITH RESPECT TO INCOME\")\nax.set(xlabel='WORKCLASS RANGE',ylabel='FREQUENCY OF WORKCLASS')\nax.legend(labels=['<=50K','>50K'],loc='upper right',fontsize='large')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#adult1=sns.load_dataset(\"adult.csv\")\ng = sns.FacetGrid(adult, col=\"occupation\")\ng.map(sns.countplot,'sex',alpha=0.7)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\n#sns.regplot(x='hours.per.week', y='fnlwgt',data=adult_new);\nx=adult_new['hours.per.week']\nplt.hist(x,bins=8,histtype='step')\nplt.ylabel('FREQUENCY')#,xlabel='Hours per week')\nplt.xlabel('HOURS PER WEEK')\nplt.title('HISTOGRAM OF HOURS PER WEEK')\nimport statistics as stat\nplt.axvline(stat.mode(x),color='red')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>Around 18K people work between 35 to 50 hours a day</p>","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(15,10))\n\nless=adult_new[adult_new['income']==0]\nage_mode1=stat.mode(less.age)\nmore=adult_new[adult_new['income']!=0]\nage_mode2=stat.mode(more.age)\n#ax.axvline(age_mode1,age_mode2)\nprint('Maximum people around age ',age_mode1,' earn <=50K \\n')\nprint('Maximum people around age ',age_mode2,' earn >50K \\n')\nax[0].hist(less['age'],bins=15,histtype='step',color='green')\nax[0].set(xlabel='AGE RANGE',ylabel='FREQUENCY OF AGE')\nax[0].set_title('AGE DISTRIBUTION FOR INCOME <=50K')\nax[0].axvline(age_mode1,color='red')\nax[1].hist(more['age'],bins=8,histtype='step',color='red')\nax[1].set(xlabel='AGE RANGE',ylabel='FREQUENCY OF AGE')\nax[1].set_title('AGE DISTRIBUTION FOR INCOME >50K')\nax[1].axvline(age_mode2,color='black')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3><p>5. FITTING APPROPRIATE MODEL</p></h3>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nchi2=adult_new1[features]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen in the first graph, there is a class imbalance problem. Models do not fit well when there is a class imbalance. There are some methods like oversampling, undersampling and mixture. I will do class oversampling using SMOTE(Synthetic Minority Oversampling  Technique). It creates new samples along the lines of the existing samples.","metadata":{}},{"cell_type":"code","source":"def train_print(clf,x_test,y_test):\n    predictions = clf.predict(x_test)\n    print('Precision report:\\nprecision \\t\\t\\t recall \\t\\t\\t f-score \\t\\t\\t support\\n',\n          precision_recall_fscore_support(y_test, predictions)[0],'\\t',\n          precision_recall_fscore_support(y_test, predictions)[1],\n          '\\t',precision_recall_fscore_support(y_test, predictions)[2],'\\t',\n          precision_recall_fscore_support(y_test, predictions)[3],'\\n')\n    print('Confusion matrix:\\n',confusion_matrix(y_test, predictions),'\\n')\n    print('Accuracy score:',accuracy_score(y_test, predictions)*100,'\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some jargons explained-\n\nprecision->true positives predicted\n\nrecall->true positives actually present\n\nf score->harmonic mean between precision and recall\n\nsupport->total number of samples in each class \n\nconfusion matrix->gives the true positives and negatives and false positives and negatives.\n\nThe aim is to increase both the precision and recall. F score gives the balance between the two.","metadata":{}},{"cell_type":"markdown","source":"<p><h3>5.1 Logistic Regression</h3></p>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nx = chi2.drop('income', axis=1)\ny = chi2['income']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=1)\nX_train, Y_train = SMOTE().fit_sample(x_train, y_train)\n\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train, Y_train)\n\ntrain_print(logmodel,x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nlr_probs = logmodel.predict_proba(x_test)\n#print(lr_probs)\n# keep probabilities for the positive outcome only\nlr_probs = lr_probs[:, 1]\n#print(lr_probs)\nns_probs = [0 for _ in range(len(y_test))]\n#print(ns_probs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\nlr_auc = roc_auc_score(y_test, lr_probs)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Logistic: ROC AUC=%.3f' % (lr_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Logistic: ROC AUC=%.3f' % (lr_auc))\n# axis labels\nplt.title('ROC CURVE')\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ROC curve gives how well the model predicted. Higher the percentage of area inside the arc better the model.","metadata":{}},{"cell_type":"markdown","source":"<p><h3>5.2 Decision Tree</h3></p>","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nx = chi2.drop('income', axis=1)\ny = chi2['income']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test\nx_train, y_train = SMOTE().fit_sample(x_train, y_train)\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion='entropy',min_samples_split=8,max_depth=10)\n\n# Train Decision Tree Classifer\nclf.fit(x_train,y_train)\n\ntrain_print(clf,x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\ndt_probs = clf.predict_proba(x_test)\n# keep probabilities for the positive outcome only\ndt_probs1 = dt_probs[:, 1]\n#lr_probs2 = lr_probs[:,0]\nns_probs = [0 for _ in range(len(y_test))]\n\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\ndt_auc = roc_auc_score(y_test, dt_probs1)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Deciison Tree: ROC AUC=%.3f' % (dt_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\ndt_fpr, dt_tpr, _ = roc_curve(y_test, dt_probs1)\n# plot the roc curve for the model\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(dt_fpr, dt_tpr, linestyle='--',marker='*',label='Deciison Tree: ROC AUC=%.3f' % (dt_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><h3>5.3 Naive Bayes</h3></p>","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n#Create a Gaussian Classifier\nmodel = GaussianNB()\n\nx = chi2.drop('income', axis=1)\ny = chi2['income']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1) # 70% training and 30% test\nx_train, y_train = SMOTE().fit_sample(x_train, y_train)\n\n# Train the model using the training sets\ngnb = model.fit(x_train,y_train)\n\ntrain_print(gnb,x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nnb_probs = model.predict_proba(x_test)\n# keep probabilities for the positive outcome only\nnb_probs1 = nb_probs[:, 1]\n#lr_probs2 = lr_probs[:,0]\nns_probs = [0 for _ in range(len(y_test))]\n\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\nnb_auc = roc_auc_score(y_test, nb_probs1)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Naive Bayes: ROC AUC=%.3f' % (nb_auc))\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nnb_fpr, nb_tpr, _ = roc_curve(y_test, nb_probs1)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(nb_fpr, nb_tpr, linestyle='--',marker='*',label='Naive Bayes: ROC AUC=%.3f' % (nb_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><h3>5.4 Random Forest</h3></p>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf=RandomForestClassifier(min_samples_split=30)\n\nx = chi2.drop('income', axis=1)\ny = chi2['income']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test\nx_train, y_train = SMOTE().fit_sample(x_train, y_train)\n\n# Train the model using the training sets\nrf.fit(x_train,y_train)\n\ntrain_print(rf,x_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict probabilities\nrf_probs = rf.predict_proba(x_test)\n# keep probabilities for the positive outcome only\nrf_probs1 = rf_probs[:, 1]\n#lr_probs2 = lr_probs[:,0]\nns_probs = [0 for _ in range(len(y_test))]\n\n# calculate scores\nns_auc = roc_auc_score(y_test, ns_probs)\nrf_auc = roc_auc_score(y_test, rf_probs1)\n# summarize scores\nprint('Random Prediction: ROC AUC=%.3f' % (ns_auc))\nprint('Random Forest: ROC AUC=%.3f' % (rf_auc))\n# calculate roc curves\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nrf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs1)\n# plot the roc curve for the model\n\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(rf_fpr, rf_tpr, linestyle='--',marker='*',label='Random Forest: ROC AUC=%.3f' % (rf_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p><h3>6. COMPARING ALL MODELS BY THEIR ROC </h3> </p>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Prediction: ROC AUC=%.3f' % (ns_auc))\nplt.plot(lr_fpr, lr_tpr, linestyle='--',marker='*', label='Logistic: ROC AUC=%.3f' % (lr_auc))\nplt.plot(dt_fpr, dt_tpr, linestyle='--',marker='*',label='Deciison Tree: ROC AUC=%.3f' % (dt_auc))\nplt.plot(nb_fpr, nb_tpr, linestyle='--',marker='*',label='Naive Bayes: ROC AUC=%.3f' % (nb_auc))\nplt.plot(rf_fpr, rf_tpr, linestyle='--',marker='*',label='Random Forest: ROC AUC=%.3f' % (rf_auc))\n# axis labels\nplt.xlabel('FALSE POSITIVE RATE')\nplt.ylabel('TRUE POSITIVE RATE')\nplt.title('ROC CURVES')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p> Random Forest performs the best with area of 0.91 under the curve.</p>","metadata":{}}]}